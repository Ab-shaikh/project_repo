services:
  # --- Database for Airflow metadata (DAG runs, users, variables) ---
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER}         # Loaded from .env
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data # Persist metadata between restarts

  # --- Airflow Webserver: serves the UI and API ---
  webserver:
    build: .                                  # Build from custom Dockerfile (Airflow + dbt)
    image: airflow_custom:latest              # Name for the built image
    depends_on: [postgres]                    # Ensure Postgres starts first
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor  # Run tasks locally (parallelizable)
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'   # Donâ€™t load Airflow example DAGs
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY} # Key for encrypting connections
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW_SQL_ALCHEMY_CONN} # Link to Postgres metadata DB
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}        # Secure sessions
      DBT_PROFILES_DIR: /opt/dbt              # Where dbt profiles.yml is mounted
      SNOWFLAKE_PASSWORD: ${SNOWFLAKE_PASSWORD} # Inject Snowflake secret
    volumes:
      - ./dags:/opt/airflow/dags             # Mount DAGs folder
      - ../data:/data/csvs:ro                # Optional: mount CSVs as read-only
      - ../dbt:/opt/dbt                      # Mount dbt project so Airflow can run it
    ports:
      - "8080:8080"                          # Expose Airflow UI at localhost:8080
    command: webserver                       # Start the webserver process

  # --- Airflow Scheduler: triggers and runs DAG tasks ---
  scheduler:
    image: airflow_custom:latest             # Use the same custom Airflow+dbt image
    depends_on: [webserver, postgres]        # Start after webserver + Postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: ${AIRFLOW_SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
      DBT_PROFILES_DIR: /opt/dbt
      SNOWFLAKE_PASSWORD: ${SNOWFLAKE_PASSWORD}
    volumes:
      - ./dags:/opt/airflow/dags
      - ../data:/data/csvs:ro
      - ../dbt:/opt/dbt
    command: scheduler
volumes:  # Named volume to persist Postgres data
  postgres_data:
